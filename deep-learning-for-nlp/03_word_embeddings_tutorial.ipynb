{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf168be5-39df-4493-b474-035fd8ccb4cc",
   "metadata": {},
   "source": [
    "# Word embeddings tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1a595-715f-4115-a156-04f380ba42db",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed0473-3b72-4ada-a673-ab8fc17769f8",
   "metadata": {},
   "source": [
    "Words can be represented as on-hot-encodings. However there are drawbacks to this representation as the words are treated as independent entities with no relation to each other. Also this requires a huge amount memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f77b1-ad72-456a-aaec-b5e84c06c168",
   "metadata": {},
   "source": [
    "We need a representation that has some notion of similarity between words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fa6c1-99f3-44dc-b022-d53a670d0d3c",
   "metadata": {},
   "source": [
    "Imagine we have the following three sentences\n",
    "\n",
    "* The mathematician ran to the store\n",
    "* The physicist ran to the store\n",
    "* The mathematician solved the open problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c145c-34ea-4d6e-8bef-7b30292b8667",
   "metadata": {},
   "source": [
    "Now suppose we get a new sentence never seen before in our data set.\n",
    "\n",
    "* The physicist solved the open problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be01a5-3742-426d-9954-329c1fab55b2",
   "metadata": {},
   "source": [
    "Our language model should recognize\n",
    "\n",
    "1. We have seen mathematician and physicist in the sam erole in a sentence\n",
    "2. We have seen mathematician in the same role as physicist in this new unseen sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90170a41-b98d-4829-8825-17e8a7156f72",
   "metadata": {},
   "source": [
    "This example relies on a fundamental linguistic assumption that words appearing in similar contexts are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8215c-7566-4a02-93c4-232cad4d02d7",
   "metadata": {},
   "source": [
    "This is called the [distributional hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2f488-5b28-4dc7-af90-d7868af78550",
   "metadata": {},
   "source": [
    "Instead of the sparse one-hot vector representation we use a dense (values typically non-zero) vector representation for mathematician $v_m$ and physicist $v_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea9e64-6289-40a2-a8cb-e0124eb70b55",
   "metadata": {},
   "source": [
    "We can find the similarity between the two values by using the consine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e3c3d-2c09-4732-826d-47ea6f1746ed",
   "metadata": {},
   "source": [
    "$$ cos(\\phi) = \\frac{ v_m \\cdot v_p }{ | v_m | \\cdot | v_p |} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f141f-24d0-4dd1-95c5-935e240b3bdc",
   "metadata": {},
   "source": [
    "We let the word embeddings be parameters in our neural network model and the model will learn the representations during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f7dd8-2e38-4936-89bb-96ec2303c17a",
   "metadata": {},
   "source": [
    "Word embeddings are an efficiently encoded semantic representation of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d329f2-65a0-44b2-92a0-8f64d8fd28ff",
   "metadata": {},
   "source": [
    "## Word embeddings in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07502036-fa96-448a-9e68-48556eedcd9e",
   "metadata": {},
   "source": [
    "Pytorch stores embeddings where each word is a unique index into a lookup table. The embeddings are stored as a $|V| \\cdot D$ matrix where $V$ is the vocabulary and $D$ is the dimensionality of the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2896eee-15da-4eea-a0a0-807c0b02afd7",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4be2b25-4d79-4dd1-8ff3-5eab9cbebda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda5c851-2bff-4160-a380-fcc33fcb2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223b239d-df7c-474e-9613-7ada35b2d909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd1b2a9b710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf1f58-64e2-48ae-b943-859a59b88e7e",
   "metadata": {},
   "source": [
    "### Example embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f886cf49-acd5-4309-9137-9f0f45e3a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9b473-a66e-4daa-a1e0-08effb156e4f",
   "metadata": {},
   "source": [
    "## Example N-Gram language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fce97-5e6d-41e4-87ae-efcea84ac36d",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9e6679-b9c2-4f9f-af70-67d0a9f46d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45835435-c6ce-4669-996b-a480fc7812f8",
   "metadata": {},
   "source": [
    "Get tuples of data\n",
    "\n",
    "([word - CONTEXT_SIZE, ... , word - 1], target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e054d1c-00fc-4937-b670-617e77d3fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = [\n",
    "    ([test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)], test_sentence[i])\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501c0a13-0c78-4b1e-a87b-d9fbfb586a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f400f0-54f4-4906-bb9e-304bba036514",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764d020-725f-4956-82b9-5861e3275d24",
   "metadata": {},
   "source": [
    "Define neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583727d5-d51b-4d51-bbf2-81f3a4a34b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 120)\n",
    "        self.linear2 = nn.Linear(120, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        logs_probs = F.log_softmax(out, dim=1)\n",
    "        return logs_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797789a-55e6-4458-af65-ee9129395317",
   "metadata": {},
   "source": [
    "Create model and set up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a3fc73-da6f-443f-b1ac-3b657e85c8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramLanguageModel(\n",
       "  (embeddings): Embedding(97, 10)\n",
       "  (linear1): Linear(in_features=20, out_features=120, bias=True)\n",
       "  (linear2): Linear(in_features=120, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec874e-766c-47c4-bf7c-0f64bdb29bcf",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d16f0ff8-a743-48d2-8a91-b5ed74e7c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523.1\n",
      "500.3\n",
      "478.7\n",
      "457.5\n",
      "436.2\n",
      "414.5\n",
      "392.1\n",
      "368.9\n",
      "344.8\n",
      "319.9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "        context_idxs = torch.tensor(\n",
    "            [word_to_ix[w] for w in context], dtype=torch.long\n",
    "        )\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(\n",
    "            log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "\n",
    "print('\\n'.join(f'{loss:.1f}' for loss in losses[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b86e2-531c-4b06-aef5-03379c1e35bc",
   "metadata": {},
   "source": [
    "Show embeddings for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c6ebe6b-59ba-4ea3-9c85-731ef022b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6702,  0.4168, -1.0248,  0.9977,  0.8020, -2.2386, -1.5171, -0.9094,\n",
      "         0.8163, -0.6731], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings.weight[word_to_ix['beauty']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216aaab5-0579-4e3b-be47-ca0025e307f3",
   "metadata": {},
   "source": [
    "## Computing Word Embeddings: Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a036a-2414-4522-800f-77db62484f40",
   "metadata": {},
   "source": [
    "The continuous bag-of-words model (CBOW) predicts words given the context of a few words before and a few words after the target word. CBOW is not sequential and does not have to be probabilistic but is typically used to train word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496d74f-f5ac-4807-8671-77ba99498d51",
   "metadata": {},
   "source": [
    "Given a target word $w_i$ and an N context window on each side, $w_{i - 1}, ..., w_{i - N}$ and $w_{i + 1}, ..., w_{i + N}$ called the context words $C$, CBOW trieds to minimize the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eca73a-7ab9-4159-9ab6-4e497875d79a",
   "metadata": {},
   "source": [
    "$$ -\\log p(w_i|C) = -\\log \\textrm{softmax} \\big ( A(\\sum \\limits_{w \\in C} {q_{w}} ) + b \\big ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3a6c22-406c-4e92-ac18-5455ca5734a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 words to the right\n",
    "raw_text = \"\"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34062f7-343d-4eb5-936c-28f93af46670",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb604165-1018-41dd-b5c3-ecd55c981b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', '\"We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "cbow_data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)] +\n",
    "        [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    cbow_data.append((context, target))\n",
    "print(cbow_data[:5])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740a4be-0b69-4f9e-9a59-f2501b2d4a1e",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35a63322-fc8e-4c88-90b5-ef89e290eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(2 * context_size * embedding_dim, 120)\n",
    "        self.linear2 = nn.Linear(120, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        logs_probs = F.log_softmax(out, dim=1)\n",
    "        return logs_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16182b7-b8e9-4131-9154-cbfb68cf5383",
   "metadata": {},
   "source": [
    "Create model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97fffa01-9c55-432b-93a0-ca540af13585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(49, 10)\n",
       "  (linear1): Linear(in_features=40, out_features=120, bias=True)\n",
       "  (linear2): Linear(in_features=120, out_features=49, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e82bc7f-aa98-47ac-8bdb-30c58a3985f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def make_target_vector(target, word_to_ix):\n",
    "    idx = word_to_ix[target]\n",
    "    return torch.tensor([idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e619e2b6-2ee9-4fce-9a8d-37093892fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.3\n",
      "218.7\n",
      "205.8\n",
      "193.1\n",
      "180.4\n",
      "167.6\n",
      "154.6\n",
      "141.5\n",
      "128.2\n",
      "114.9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in cbow_data:\n",
    "        context_idxs = make_context_vector(context, word_to_ix)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(\n",
    "            log_probs, make_target_vector(target, word_to_ix)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "\n",
    "print('\\n'.join(f'{loss:.1f}' for loss in losses[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ea82d-e5aa-432e-9947-3116fdd763e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
